{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Establishing Baseline (All Features Included) -----\n",
      "Extracting features and labels from data/pre.conll2003.train_small.conll, omitting feature: None\n",
      "Extracted 10000 data points.\n",
      "Extracting features and labels from data/pre.conll2003.dev_small.conll, omitting feature: None\n",
      "Extracted 10000 data points.\n",
      "Training and evaluating SVM...\n",
      "Model evaluation completed. F1 Score: 0.8693131600069762\n",
      "Baseline F1 Score (All Features): 0.8693131600069762\n",
      "\n",
      "----- Ablation study for feature: OriginalWord -----\n",
      "Extracting features and labels from data/pre.conll2003.train_small.conll, omitting feature: OriginalWord\n",
      "Extracted 10000 data points.\n",
      "Extracting features and labels from data/pre.conll2003.dev_small.conll, omitting feature: OriginalWord\n",
      "Extracted 10000 data points.\n",
      "Training and evaluating SVM...\n",
      "Model evaluation completed. F1 Score: 0.8675746246721072\n",
      "\n",
      "----- Ablation study for feature: PreviousWord -----\n",
      "Extracting features and labels from data/pre.conll2003.train_small.conll, omitting feature: PreviousWord\n",
      "Extracted 10000 data points.\n",
      "Extracting features and labels from data/pre.conll2003.dev_small.conll, omitting feature: PreviousWord\n",
      "Extracted 10000 data points.\n",
      "Training and evaluating SVM...\n",
      "Model evaluation completed. F1 Score: 0.8558871047481411\n",
      "\n",
      "----- Ablation study for feature: NextWord -----\n",
      "Extracting features and labels from data/pre.conll2003.train_small.conll, omitting feature: NextWord\n",
      "Extracted 10000 data points.\n",
      "Extracting features and labels from data/pre.conll2003.dev_small.conll, omitting feature: NextWord\n",
      "Extracted 10000 data points.\n",
      "Training and evaluating SVM...\n",
      "Model evaluation completed. F1 Score: 0.8589492268662018\n",
      "\n",
      "----- Ablation study for feature: Lemma -----\n",
      "Extracting features and labels from data/pre.conll2003.train_small.conll, omitting feature: Lemma\n",
      "Extracted 10000 data points.\n",
      "Extracting features and labels from data/pre.conll2003.dev_small.conll, omitting feature: Lemma\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def extract_features_and_labels(file_path, include_embeddings=True, omit_feature=None):\n",
    "    print(f\"Extracting features and labels from {file_path}, omitting feature: {omit_feature}\")\n",
    "    data = []\n",
    "    data = []\n",
    "    targets = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(\"embeddings/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            components = line.rstrip('\\n').split()\n",
    "            if len(components) == 10:\n",
    "                token, preceding_token, next_token, lemma, capitalization, word_shape, word_length, pos_tag, chunk_label, gold_label = components\n",
    "\n",
    "                # Prepare embeddings only if needed\n",
    "                embedding_features = {}\n",
    "                if include_embeddings and omit_feature != 'embeddings':\n",
    "                    # Get embedding if available, else use a zero vector\n",
    "                    embedding = word_vectors[token] if token in word_vectors else [0]*300\n",
    "                    embedding_features = {f'emb_{i}': emb for i, emb in enumerate(embedding)}\n",
    "\n",
    "                feature_dict = {\n",
    "                    'OriginalWord': token,\n",
    "                    'PreviousWord': preceding_token,\n",
    "                    'NextWord': next_token,\n",
    "                    'Lemma': lemma,\n",
    "                    'Capitalization': capitalization,\n",
    "                    'WordShape': word_shape,\n",
    "                    'WordLength': str(word_length),\n",
    "                    'POS': pos_tag,\n",
    "                    'ChunkTag': chunk_label\n",
    "                }\n",
    "\n",
    "                # Remove omitted feature, if any\n",
    "                if omit_feature in feature_dict:\n",
    "                    del feature_dict[omit_feature]\n",
    "                    #print(\"deleted \",omit_feature)\n",
    "\n",
    "                # Add embedding features if they are to be included\n",
    "                if omit_feature != 'embeddings':\n",
    "                    feature_dict.update(embedding_features)\n",
    "\n",
    "                data.append(feature_dict)\n",
    "                targets.append(gold_label)\n",
    "\n",
    "    print(f\"Extracted {len(data)} data points.\")\n",
    "\n",
    "    return data, targets\n",
    "\n",
    "# Train and evaluate the SVM model\n",
    "def train_and_evaluate_svm(train_features, train_labels, dev_features, dev_labels):\n",
    "    print(\"Training and evaluating SVM...\")\n",
    "    vec = DictVectorizer()\n",
    "    X_train = vec.fit_transform(train_features)\n",
    "    X_dev = vec.transform(dev_features)\n",
    "\n",
    "    model = SVC(C=1, gamma=0.001, kernel='linear', class_weight= \"balanced\")\n",
    "    model.fit(X_train, train_labels)\n",
    "\n",
    "    predictions = model.predict(X_dev)\n",
    "    f1 = f1_score(dev_labels, predictions, average='weighted')\n",
    "    print(f\"Model evaluation completed. F1 Score: {f1}\")\n",
    "    return f1\n",
    "\n",
    "\n",
    "# Load your datasets\n",
    "train_file_path = \"data/pre.conll2003.train_small.conll\"\n",
    "dev_file_path = \"data/pre.conll2003.dev_small.conll\"\n",
    "\n",
    "\n",
    "# Establish baseline with all features\n",
    "print(\"\\n----- Establishing Baseline (All Features Included) -----\")\n",
    "train_features, train_labels = extract_features_and_labels(train_file_path)\n",
    "dev_features, dev_labels = extract_features_and_labels(dev_file_path)\n",
    "baseline_f1 = train_and_evaluate_svm(train_features, train_labels, dev_features, dev_labels)\n",
    "print(f\"Baseline F1 Score (All Features): {baseline_f1}\")\n",
    "\n",
    "# Perform ablation study\n",
    "f1_scores = {\"Baseline (All Features)\": baseline_f1}\n",
    "\n",
    "for feature_to_omit in features:\n",
    "    print(f\"\\n----- Ablation study for feature: {feature_to_omit} -----\")\n",
    "    train_features, train_labels = extract_features_and_labels(train_file_path, omit_feature=feature_to_omit)\n",
    "    dev_features, dev_labels = extract_features_and_labels(dev_file_path, omit_feature=feature_to_omit)\n",
    "\n",
    "    f1 = train_and_evaluate_svm(train_features, train_labels, dev_features, dev_labels)\n",
    "    f1_scores[feature_to_omit] = f1\n",
    "print(\"\\nFeature Ablation Study Results:\")\n",
    "# Analyzing and Ranking Features\n",
    "sorted_features = sorted(f1_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature Ablation Study Results:\")\n",
    "for feature, f1 in sorted_features:\n",
    "    print(f\"Feature: {feature}, F1 Score: {f1}\")\n",
    "\n",
    "# Identify the most and least important features based on F1 score\n",
    "most_important_feature = sorted_features[0][0]\n",
    "least_important_feature = sorted_features[-1][0]\n",
    "\n",
    "print(f\"\\nMost Important Feature: {most_important_feature}\")\n",
    "print(f\"Least Important Feature: {least_important_feature}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
